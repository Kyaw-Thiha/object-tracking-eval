# Project Structure

```
object-tracking-eval/
├── docker/
│   └── ...
├── docs/
│   ├── codebase_navigation.md
│   ├── dataset_format.md
│   ├── evaluation_metrics.md
│   └── ...
├── src/
│   ├── configs/
│   │   └── ...
│   ├── core/
│   │   └── ...
│   ├── dataloader_factory/
│   │   └── ...
│   ├── datasets/
│   │   └── ...
│   ├── evaluation_metrics/
│   │   ├── AssA.py
│   │   ├── detA.py
│   │   ├── hota.py
│   │   ├── evaluate.py
│   │   └── ...
│   ├── model/
│   │   ├── det/
│   │   │   └── yolox/
│   │   └── tracker/
│   │       └── ...
│   ├── model_factory/
│   │   ├── prob_yolox.py
│   │   └── ...
│   ├── evaluation_pipeline.py
│   ├── evaluation_pipeline.sh
│   ├── plot_evaluation_results.py
│   ├── train.py
│   └── ...
├── README.md
└── LICENSE
```

- `docs/`: reference material covering dataset format, evaluation metrics, and navigation guidance.
- `src/evaluation_pipeline.py`: main multi-object tracking evaluation driver (arg parsing, dynamic loading, inference, metrics).
- `src/evaluation_pipeline.sh`: convenience script that wraps the Python pipeline invocation.
- `data/`: COCO-style dataset files (e.g., CAMEL, MOT17).
- `src/datasets/`: PyTorch dataset definitions that wrap the raw data.
- `src/dataloader_factory/`: factories that instantiate datasets/data loaders so the pipeline can dynamically select datasets.
- `src/model_factory/`: detector factory modules (YOLOX variants) that expose `factory()` and `infer()` interfaces.
- `src/model/tracker/`: tracker implementations reused from UncertaintyTrack.
- `src/evaluation_metrics/`: logic for DetA, AssA, HOTA, and evaluation orchestration.
- `src/configs/`: detector/training configuration files (e.g., `prob_yolox_x_es_mot17-half.py`).
- `src/plot_evaluation_results.py`: helper for plotting comparative evaluation bar charts.
- `docker/`: container scripts/assets for reproducible environments (usually untouched unless rebuilding images).

## Artifact Folders
The following are artifact folders that are not version-controlled.
All these folders are expected to be in the root of the project.

- `data/`: root-level storage for COCO-style datasets (MOT17, CAMEL, BDD, CrowdHuman). All dataloader factories and evaluation scripts resolve images/annotations from here by default.
- `results/`: experiment work directories produced by training/eval scripts (`train.bash`, `eval_*.bash`). Each config writes its checkpoints, logs, and eval outputs under `results/<exp_name>`.
- `outputs/`: tracker inference dumps and miscellaneous artifacts used by downstream analysis (diffing, visualization, metrics). Scripts like `evaluate_model.py`, `diff_tracking_results.py`, and the evaluation metrics read/write under this folder.
- `evaluation_results/`: aggregated metrics or reports generated by `evaluation_pipeline.sh` when `--eval_result_dir` is provided.
- `plots/`: saved figures/diagnostics (e.g., diff plots, stability plots). Analysis scripts append files under `plots/…`.
- `checkpoints/`: pretrained detector weights or experiment checkpoints consumed by configs and model factories (via `CHECKPOINT_ROOT`).
- `work_dirs/`: MMDetection-style training outputs; configs reference `WORK_DIR_ROOT` to resume from `work_dirs/<run>/latest.pth`.

If you cloned this repo, then u will need to 
1. Manually create the `data` folder, and download the dataset as `data/<dataset_name>/`
2. Manually create the `checkpoints` folder and download the checkpoints as `checkpoints/<name>.pth`
3. (Optional) Pre-create the `results`, `outputs`, `evaluation_results` and `plots` folders.
